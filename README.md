# A Qualitative Study on XAI Techniques for Software Defect Prediction
Machine learning (ML) models are widely used in software defect prediction (SDP), but their lack of transparency has led to growing interest in explainable AI (XAI) techniques. While tools like LIME, SHAP, PyExplainer, and BreakDown aim to make model outputs more interpretable, developers often face inconsistent or conflicting explanations that hinder trust and adoption. In this study, we explore how XAI is used in SDP through a mixed-methods approach, combining a systematic literature review (SLR) of 93 papers, a comparative evaluation of popular XAI tools, and a scenario-based analysis using a standard defect prediction model. We also conduct a user study with 71 practitioners to gather real-world perspectives. Our findings reveal significant disagreement across XAI tools in terms of feature importance, ranking, and sign interpretation. Developers report usability issues, including confusion, integration challenges, and a lack of reliable guidance. To address this, we propose agreement-based metrics and offer practical recommendations for more trustworthy and consistent XAI tools in software engineering (SE). This work contributes a grounded understanding of the current landscape and challenges in applying XAI to defect prediction tasks.
